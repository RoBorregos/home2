services:
  ollama:
    profiles: [receptionist, carry, gpsr, storing]
    container_name: home2-hri-ollama-cpu
    build:
      context: ..
      dockerfile: dockerfiles/Dockerfile.ollama
    image: roborregos/home2:hri-ollama-cpu
    # No nvidia runtime for CPU/macOS
    ports:
      - "11434:11434"  # Expose Ollama API port
    volumes:
      - ../../../hri/packages/nlp/assets:/ollama
    environment:
      OLLAMA_MODELS: /ollama
      ROLE: ${ROLE}
    stdin_open: true
    tty: true
    entrypoint:
      ["/bin/bash", "-c", "/ollama/entrypoint.sh && tail -f /dev/null"]
