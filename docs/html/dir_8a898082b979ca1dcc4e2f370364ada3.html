<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.15.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Roborregos @Home documentation: /home/deivideich/roborregos/home_ws/src/home2/manipulation/packages/gpd Directory Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Roborregos @Home documentation<span id="projectnumber">&#160;1.0.0</span>
   </div>
   <div id="projectbrief">Class documentation for the Roborregos @Home project</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.15.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('dir_8a898082b979ca1dcc4e2f370364ada3.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">gpd Directory Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-subdirs" class="groupheader"><a id="subdirs" name="subdirs"></a>
Directories</h2></td></tr>
<tr class="memitem:contrib" id="r_contrib"><td class="memItemLeft" align="right" valign="top"><span class="iconfolder"><div class="folder-icon"></div></span>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="dir_5060c6a78b1ffe12bbf0a0eb83ac9896.html">contrib</a></td></tr>
<tr class="memitem:include" id="r_include"><td class="memItemLeft" align="right" valign="top"><span class="iconfolder"><div class="folder-icon"></div></span>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="dir_190fe14b957097bdc4cc3823475aff9e.html">include</a></td></tr>
<tr class="memitem:pytorch" id="r_pytorch"><td class="memItemLeft" align="right" valign="top"><span class="iconfolder"><div class="folder-icon"></div></span>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="dir_3a75f88eeeef6e41a35619a02a5f6729.html">pytorch</a></td></tr>
<tr class="memitem:src" id="r_src"><td class="memItemLeft" align="right" valign="top"><span class="iconfolder"><div class="folder-icon"></div></span>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="dir_f4eecd8631ff53bc6ed642c4f220b256.html">src</a></td></tr>
</table>
<a name="details" id="details"></a><h2 id="header-details" class="groupheader">Detailed Description</h2>
<h1 class="doxsection"><a class="anchor" id="autotoc_md24"></a>
Grasp Pose Detection (GPD)</h1>
<ul>
<li><a href="http://www.ccs.neu.edu/home/atp/">Author's website</a></li>
<li><a href="https://github.com/atenpas/gpd/blob/master/LICENSE.md">License</a></li>
<li><a href="https://github.com/atenpas/gpd_ros/">ROS wrapper</a></li>
</ul>
<p>Grasp Pose Detection (GPD) is a package to detect 6-DOF grasp poses (3-DOF position and 3-DOF orientation) for a 2-finger robot hand (e.g., a parallel jaw gripper) in 3D point clouds. GPD takes a point cloud as input and produces pose estimates of viable grasps as output. The main strengths of GPD are:</p><ul>
<li>works for novel objects (no CAD models required for detection),</li>
<li>works in dense clutter, and</li>
<li>outputs 6-DOF grasp poses (enabling more than just top-down grasps).</li>
</ul>
<p><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=kfe5bNt35ZI
" target="_blank"><img src="readme/ur5_video.jpg" alt="UR5 demo" width="320" height="240" border="0" class="inline"/></a></p>
<p>GPD consists of two main steps: sampling a large number of grasp candidates, and classifying these candidates as viable grasps or not.</p>
<h4 class="doxsection"><a class="anchor" id="autotoc_md25"></a>
Example Input and Output</h4>
<p><img src="readme/clutter.png" alt="" height="170px/" class="inline"/></p>
<p>The reference for this package is: <a href="http://arxiv.org/abs/1706.09911">Grasp Pose Detection in Point Clouds</a>.</p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md26"></a>
Table of Contents</h1>
<ol type="1">
<li><a class="el" href="#requirements">Requirements</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#install">Installation</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#pcd">Generate Grasps for a Point Cloud File</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#parameters">Parameters</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#views">Views</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#cnn_channels">Input Channels for Neural Network</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#cnn_frameworks">CNN Frameworks</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#net_train">Network Training</a></li>
</ol>
<ol type="1">
<li><a class="el" href="#descriptor">Grasp Image</a></li>
</ol>
<ol type="1">
<li>References</li>
</ol>
<ol type="1">
<li><a class="el" href="#troubleshooting">Troubleshooting</a></li>
</ol>
<p><a class="anchor" id="requirements"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md27"></a>
1) Requirements</h1>
<ol type="1">
<li><a href="http://pointclouds.org/">PCL 1.9 or newer</a></li>
<li><a href="https://eigen.tuxfamily.org">Eigen 3.0 or newer</a></li>
<li><a href="https://opencv.org">OpenCV 3.4 or newer</a></li>
</ol>
<p><a class="anchor" id="install"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md28"></a>
2) Installation</h1>
<p>The following instructions have been tested on <b>Ubuntu 16.04</b>. Similar instructions should work for other Linux distributions.</p>
<ol type="1">
<li>Install <a href="http://pointclouds.org/">PCL</a> and <a href="https://eigen.tuxfamily.org">Eigen</a>. If you have ROS Indigo or Kinetic installed, you should be good to go.</li>
<li>Install OpenCV 3.4 (<a href="https://www.python36.com/how-to-install-opencv340-on-ubuntu1604/">tutorial</a>).</li>
<li><p class="startli">Clone the repository into some folder:</p>
<div class="fragment"><div class="line">git clone https://github.com/atenpas/gpd</div>
</div><!-- fragment --></li>
<li><p class="startli">Build the package:</p>
<div class="fragment"><div class="line">cd gpd</div>
<div class="line">mkdir build &amp;&amp; cd build</div>
<div class="line">cmake ..</div>
<div class="line">make -j</div>
</div><!-- fragment --></li>
</ol>
<p>You can optionally install GPD with <span class="tt">sudo make install</span> so that it can be used by other projects as a shared library.</p>
<p>If building the package does not work, try to modify the compiler flags, <span class="tt">CMAKE_CXX_FLAGS</span>, in the file CMakeLists.txt.</p>
<p><a class="anchor" id="pcd"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md29"></a>
3) Generate Grasps for a Point Cloud File</h1>
<p>Run GPD on an point cloud file (PCD or PLY):</p>
<div class="fragment"><div class="line">./detect_grasps ../cfg/eigen_params.cfg ../tutorials/krylon.pcd</div>
</div><!-- fragment --><p>The output should look similar to the screenshot shown below. The window is the PCL viewer. You can press [q] to close the window and [h] to see a list of other commands.</p>
<p><img src="readme/file.png" alt="" width="30%" border="0" class="inline"/></p>
<p>Below is a visualization of the convention that GPD uses for the grasp pose (position and orientation) of a grasp. The grasp position is indicated by the orange cross and the orientation by the colored arrows.</p>
<p><img src="readme/hand_frame.png" alt="" width="30%" border="0" class="inline"/></p>
<p><a class="anchor" id="parameters"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md30"></a>
4) Parameters</h1>
<p>Brief explanations of parameters are given in <a href="cfg/eigen_params.cfg">cfg/eigen_params.cfg</a>.</p>
<p>The two parameters that you typically want to play with to <b>improve the number of grasps found</b> are <em>workspace</em> and <em>num_samples</em>. The first defines the volume of space in which to search for grasps as a cuboid of dimensions [minX, maxX, minY, maxY, minZ, maxZ], centered at the origin of the point cloud frame. The second is the number of samples that are drawn from the point cloud to detect grasps. You should set the workspace as small as possible and the number of samples as large as possible.</p>
<p>Most of the code is parallelized. To <b>improve runtime</b>, set <em>num_threads</em> to the number of (physical) CPU cores that your computer has available.</p>
<p><a class="anchor" id="views"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md31"></a>
5) Views</h1>
<p><img src="readme/views.png" alt="rviz screenshot" title="Single View and Two Views" class="inline"/></p>
<p>You can use this package with a single or with two depth sensors. The package comes with CAFFE model files for both. You can find these files in <em>models/caffe/15channels</em>. For a single sensor, use <em>single_view_15_channels.caffemodel</em> and for two depth sensors, use <em>two_views_15_channels_[angle]</em>. The <em>[angle]</em> is the angle between the two sensor views, as illustrated in the picture below. In the two-views setting, you want to register the two point clouds together before sending them to GPD.</p>
<p>Providing the camera position to the configuration file (*.cfg) is important, as it enables PCL to estimate the correct normals direction (which is to point toward the camera). Alternatively, using the <a href="https://github.com/atenpas/gpd_ros/">ROS wrapper</a>, multiple camera positions can be provided.</p>
<p><img src="readme/view_angle.png" alt="rviz screenshot" title="Angle Between Sensor Views" class="inline"/></p>
<p>To switch between one and two sensor views, change the parameter <span class="tt">weight_file</span> in your config file.</p>
<p><a class="anchor" id="cnn_channels"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md32"></a>
6) Input Channels for Neural Network</h1>
<p>The package comes with weight files for two different input representations for the neural network that is used to decide if a grasp is viable or not: 3 or 15 channels. The default is 15 channels. However, you can use the 3 channels to achieve better runtime for a loss in grasp quality. For more details, please see the references below.</p>
<p><a class="anchor" id="cnn_frameworks"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md33"></a>
7) CNN Frameworks</h1>
<p>GPD comes with a number of different classifier frameworks that exploit different hardware and have different dependencies. Switching between the frameworks requires to run CMake with additional arguments. For example, to use the OpenVino framework:</p>
<div class="fragment"><div class="line">cmake .. -DUSE_OPENVINO=ON</div>
</div><!-- fragment --><p>You can use <span class="tt">ccmake</span> to check out all possible CMake options.</p>
<p>GPD supports the following three frameworks:</p>
<ol type="1">
<li><a href="https://software.intel.com/en-us/openvino-toolkit">OpenVino</a>: <a href="https://github.com/opencv/dldt/blob/2018/inference-engine/README.md">installation instructions</a> for open source version (CPUs, GPUs, FPGAs from Intel)</li>
</ol>
<ol type="1">
<li><a href="https://caffe.berkeleyvision.org/">Caffe</a> (GPUs from Nvidia or CPUs)</li>
</ol>
<ol type="1">
<li>Custom LeNet implementation using the Eigen library (CPU)</li>
</ol>
<p>Additional classifiers can be added by sub-classing the <span class="tt">classifier</span> interface.</p>
<h4 class="doxsection"><a class="anchor" id="autotoc_md34"></a>
OpenVINO</h4>
<p>OpenVINO is <b>recommended for speed</b>. To use OpenVINO, you need to run the following command before compiling GPD.</p>
<div class="fragment"><div class="line">export InferenceEngine_DIR=/path/to/dldt/inference-engine/build/</div>
</div><!-- fragment --><p><a class="anchor" id="net_train"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md35"></a>
8) Network Training</h1>
<p>To create training data with the C++ code, you need to install <a href="https://www.python36.com/how-to-install-opencv340-on-ubuntu1604/">OpenCV 3.4 Contribs</a>. Next, you need to compile GPD with the flag <span class="tt">DBUILD_DATA_GENERATION</span> like this: </p><pre class="fragment">```
cd gpd
mkdir build &amp;&amp; cd build
cmake .. -DBUILD_DATA_GENERATION=ON
make -j
```
</pre><p>There are four steps to train a network to predict grasp poses. First, we need to create grasp images.</p>
<div class="fragment"><div class="line">./generate_data ../cfg/generate_data.cfg</div>
</div><!-- fragment --><p>You should modify <span class="tt">generate_data.cfg</span> according to your needs.</p>
<p>Next, you need to resize the created databases to <span class="tt">train_offset</span> and <span class="tt">test_offset</span> (see the terminal output of <span class="tt">generate_data</span>). For example, to resize the training set, use the following commands with <span class="tt">size</span> set to the value of <span class="tt">train_offset</span>. </p><div class="fragment"><div class="line">cd pytorch</div>
<div class="line">python reshape_hdf5.py pathToTrainingSet.h5 out.h5 size</div>
</div><!-- fragment --><p>The third step is to train a neural network. The easiest way to training the network is with the existing code. This requires the <b>pytorch</b> framework. To train a network, use the following commands.</p>
<div class="fragment"><div class="line">cd pytorch</div>
<div class="line">python train_net3.py pathToTrainingSet.h5 pathToTestSet.h5 num_channels</div>
</div><!-- fragment --><p>The fourth step is to convert the model to the ONNX format.</p>
<div class="fragment"><div class="line">python torch_to_onxx.py pathToPytorchModel.pwf pathToONNXModel.onnx num_channels</div>
</div><!-- fragment --><p>The last step is to convert the ONNX file to an OpenVINO compatible format: <a href="https://software.intel.com/en-us/articles/OpenVINO-Using-ONNX#inpage-nav-4">tutorial</a>. This gives two files that can be loaded with GPD by modifying the <span class="tt">weight_file</span> and <span class="tt">model_file</span> parameters in a CFG file.</p>
<p><a class="anchor" id="descriptor"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md36"></a>
9) Grasp Image/Descriptor</h1>
<p>Generate some grasp poses and their corresponding images/descriptors:</p>
<div class="fragment"><div class="line">./test_grasp_image ../tutorials/krylon.pcd 3456 1 ../models/lenet/15channels/params/</div>
</div><!-- fragment --><p><img src="readme/image_15channels.png" alt="" width="30%" border="0" class="inline"/></p>
<p>For details on how the grasp image is created, check out our <a href="http://arxiv.org/abs/1706.09911">journal paper</a>.</p>
<p><a class="anchor" id="references"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md37"></a>
10) References</h1>
<p>If you like this package and use it in your own work, please cite our journal paper [1]. If you're interested in the (shorter) conference version, check out [2].</p>
<p>[1] Andreas ten Pas, Marcus Gualtieri, Kate Saenko, and Robert Platt. <a href="http://arxiv.org/abs/1706.09911"><b>Grasp Pose Detection in Point Clouds</b></a>. The International Journal of Robotics Research, Vol 36, Issue 13-14, pp. 1455-1473. October 2017.</p>
<p>[2] Marcus Gualtieri, Andreas ten Pas, Kate Saenko, and Robert Platt. [<b>High precision grasp pose detection in dense clutter</b>](<a href="http://arxiv.org/abs/1603.01564">http://arxiv.org/abs/1603.01564</a>). IROS 2016, pp. 598-605.</p>
<p><a class="anchor" id="troubleshooting"></a> </p>
<h1 class="doxsection"><a class="anchor" id="autotoc_md38"></a>
11) Troubleshooting Tips</h1>
<ol type="1">
<li>Remove the <span class="tt">cmake</span> cache: <span class="tt">rm CMakeCache.txt</span></li>
</ol>
<ol type="1">
<li><span class="tt">make clean</span></li>
</ol>
<ol type="1">
<li>Remove the <span class="tt">build</span> folder and rebuild.</li>
</ol>
<ol type="1">
<li>Update <em>gcc</em> and <em>g++</em> to a version &gt; 5. </li>
</ol>
</div><!-- contents -->
</div><!-- doc-content -->
<div id="page-nav" class="page-nav-panel">
<div id="page-nav-resize-handle"></div>
<div id="page-nav-tree">
<div id="page-nav-contents">
</div><!-- page-nav-contents -->
</div><!-- page-nav-tree -->
</div><!-- page-nav -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a href="dir_2d3b31f5269641a65ad69fd5e63e7c5f.html">manipulation</a></li><li class="navelem"><a href="dir_7789cadc33ffb2720130327e1e043559.html">packages</a></li><li class="navelem"><a href="dir_8a898082b979ca1dcc4e2f370364ada3.html">gpd</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.15.0 </li>
  </ul>
</div>
</body>
</html>
